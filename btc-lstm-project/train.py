import torch
import torch.nn as nn
import torch.optim as optim
import os
import numpy as np
from sklearn.preprocessing import StandardScaler
from data_utils import fetch_multi_data, create_sequences, save_scaler, TICKERS
from model import LSTMModel, DLinear, PatchTST, iTransformer, TCN, MLP

# GPU/CPU ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu")
print(f"âœ… í•™ìŠµ ì¥ì¹˜: {device}")

# =============================================================================
# 1. ë°ì´í„° ì¤€ë¹„ ë° ì „ì²˜ë¦¬ (Data Leakage ë°©ì§€ ì ìš©)
# =============================================================================
print("ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬ ì¤‘...")
df = fetch_multi_data()
features = list(TICKERS.keys())
btc_idx = features.index('BTC_Close') # ë˜ëŠ” 'Bitcoin' (data_utils.py ì„¤ì •ì— ë”°ë¦„)

# [í•µì‹¬ ìˆ˜ì •] ìŠ¤ì¼€ì¼ë§ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ ë¡œì§
# ì „ì²´ ë°ì´í„°ë¡œ fitì„ í•˜ë©´ ë¯¸ë˜ ì •ë³´(í‰ê· , ë¶„ì‚°)ê°€ ë°˜ì˜ë˜ë¯€ë¡œ,
# ê³¼ê±° ë°ì´í„°(ì•ìª½ 90%)ë¡œë§Œ ê¸°ì¤€ì„ ì¡ê³ (fit), ë³€í™˜(transform)ì€ ì „ì²´ì— ëŒ€í•´ ìˆ˜í–‰í•©ë‹ˆë‹¤.
train_split_idx = int(len(df) * 0.9)
train_data_for_scaler = df[features].iloc[:train_split_idx]

scaler = StandardScaler()
scaler.fit(train_data_for_scaler) # âš ï¸ ê³¼ê±° ë°ì´í„°ë¡œë§Œ í•™ìŠµ!
scaled_data = scaler.transform(df[features]) # ë³€í™˜ì€ ì „ì²´ ë°ì´í„° ì ìš©
save_scaler(scaler)

input_dim = len(features)
prediction_days = 7

# =============================================================================
# 2. ëª¨ë¸ ì„¤ì • (14, 21, 45ì¼ ë©€í‹° ì‹œí€€ìŠ¤)
# =============================================================================
# ë‹¤ì–‘í•œ ê´€ì (ë‹¨ê¸°/ì¤‘ê¸°/ì¥ê¸°)ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ì‹œí€€ìŠ¤ ê¸¸ì´ ëª©ë¡
SEQ_LENGTHS = [14, 21, 45]
model_names = ["MLP", "DLinear", "TCN", "LSTM", "PatchTST", "iTransformer"]

# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± íŒ©í† ë¦¬ í•¨ìˆ˜
def get_model_instance(name, seq_len):
    if name == "MLP": 
        return MLP(seq_len=seq_len, input_size=input_dim, pred_len=prediction_days, hidden_sizes=[256, 128], dropout=0.1)
    elif name == "DLinear": 
        return DLinear(seq_len=seq_len, pred_len=prediction_days, input_size=input_dim, kernel_size=25)
    elif name == "TCN": 
        return TCN(input_size=input_dim, output_size=prediction_days, num_channels=[64, 64, 64], kernel_size=3, dropout=0.2)
    elif name == "LSTM": 
        return LSTMModel(input_size=input_dim, output_size=prediction_days)
    elif name == "PatchTST": 
        return PatchTST(input_size=input_dim, seq_len=seq_len, pred_len=prediction_days,
                        patch_len=7, stride=3, d_model=64, n_heads=4, n_layers=2, dropout=0.2)
    elif name == "iTransformer": 
        return iTransformer(seq_len=seq_len, pred_len=prediction_days, input_size=input_dim,
                            d_model=256, n_heads=4, n_layers=3, dropout=0.2)
    return None

# =============================================================================
# 3. í•™ìŠµ ë£¨í”„ (ì‹œí€€ìŠ¤ ê¸¸ì´ x ëª¨ë¸ ì¢…ë¥˜)
# =============================================================================
os.makedirs('weights', exist_ok=True)
batch_size = 64

print(f"ğŸš€ ì´ {len(SEQ_LENGTHS) * len(model_names)}ê°œì˜ ëª¨ë¸ í•™ìŠµì´ ì§„í–‰ë©ë‹ˆë‹¤.")

for seq_len in SEQ_LENGTHS:
    print(f"\n{'='*50}")
    print(f"ğŸ“… Lookback Window: {seq_len}ì¼ ë°ì´í„°ì…‹ ìƒì„±")
    print(f"{'='*50}")
    
    # í•´ë‹¹ ê¸¸ì´ì— ë§ëŠ” ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
    # (ì£¼ì˜: í•™ìŠµì€ 'ì „ì²´ ë°ì´í„°'ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì‹  ê²½í–¥ê¹Œì§€ ë°˜ì˜í•©ë‹ˆë‹¤)
    X, y = create_sequences(scaled_data, seq_len, prediction_days=prediction_days, target_col_idx=btc_idx)
    X_train, y_train = torch.tensor(X).float(), torch.tensor(y).float()
    
    for name in model_names:
        # ëª¨ë¸ë³„ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš© (Epochs, LR)
        if name == "TCN":
            epochs, lr = 200, 0.005
        elif name in ["PatchTST", "iTransformer"]:
            epochs, lr = 150, 0.001
        else:
            epochs, lr = 100, 0.005 # MLP, LSTM, DLinear ë“±

        print(f"ğŸš€ [{name}] (Seq: {seq_len}) í•™ìŠµ ì‹œì‘... (Epochs: {epochs})")
        
        # ëª¨ë¸ ì´ˆê¸°í™” & GPU ì´ë™
        model = get_model_instance(name, seq_len)
        model.to(device)
        
        optimizer = optim.Adam(model.parameters(), lr=lr)
        criterion = nn.MSELoss()
        
        # í•™ìŠµ ìˆ˜í–‰
        for epoch in range(epochs):
            model.train()
            permutation = torch.randperm(X_train.size()[0])
            epoch_loss = 0
            
            for i in range(0, X_train.size()[0], batch_size):
                indices = permutation[i:i+batch_size]
                batch_x, batch_y = X_train[indices].to(device), y_train[indices].to(device)
                
                optimizer.zero_grad()
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # ê¸°ìš¸ê¸° í­ë°œ ë°©ì§€
                optimizer.step()
                epoch_loss += loss.item()
                
            # ë¡œê·¸ ì¶œë ¥ (ì§„í–‰ ìƒí™© í™•ì¸ìš©)
            if (epoch + 1) % 50 == 0:
                avg_loss = epoch_loss / (len(X_train) / batch_size)
                print(f"   Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.6f}")

        # ê°€ì¤‘ì¹˜ íŒŒì¼ ì €ì¥ (íŒŒì¼ëª… í˜•ì‹: ëª¨ë¸ëª…_ì‹œí€€ìŠ¤ê¸¸ì´.pth)
        save_path = f'weights/{name}_{seq_len}.pth'
        torch.save(model.cpu().state_dict(), save_path)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_path}")

print("\nğŸ‰ ëª¨ë“  í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
